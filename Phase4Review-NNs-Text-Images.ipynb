{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Review and Practice Session!\n",
    "\n",
    "Using both Text and Image data with Neural Networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a results visualization function\n",
    "def visualize_training_results(history):\n",
    "    '''\n",
    "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    \n",
    "    Input: keras history object (output from trained model)\n",
    "    '''\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig.suptitle('Model Results')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Some Resources/References/Cheat Codes for Tuning Neural Networks\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/st83jeYy9L6Bq/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Adding nodes and layers\n",
    "\n",
    "- Number of hidden layers\n",
    "\n",
    "> For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification without convolutional layers, will need dozens of layers.\n",
    "\n",
    "- Number of neurons per layer\n",
    "\n",
    "> The number of neurons for the input and output layers is dependent on your data and the task. i.e. input dimensions are determined by your number or columns and your output layer for classification has just one node with a sigmoid activation function. For hidden layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.\n",
    "\n",
    "\n",
    "In general, you will get more bang for your buck by adding on more layers than adding more neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Activation functions\n",
    "\n",
    "* One thing important for activation functions is its **differentiability** because the derivative is used in the backpropagation process\n",
    "\n",
    "<img src='images/activation.png' width=500/>\n",
    "\n",
    "#### Activation functions for output layers (for supervised learning problems)\n",
    "\n",
    "1. For binary classification problems: sigmoid activation to coerce values between 0-1\n",
    "2. For multiclass classification: softmax activation, as it produces a non-negative vector that sums to 1 (probabilities of your test point belonging to the different classes)\n",
    "3. For regression problems: linear, or relu activation (it is linear and unbounded!)\n",
    "\n",
    "#### Activation functions for hidden layers\n",
    "\n",
    "Relevant/useful blog posts (by different authors, despite the similar titles): \n",
    "- [Exploring Activation Functions for Neural Networks](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)\n",
    "    - Discusses why adding non-linear activation functions helps us solve non-linear problems, as well as why an understanding of the derivatives of these activation functions helps us tune NNs more effectively\n",
    "- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "    - Builds the intuitition behind activation functions (despite less-than-stellar grammar and punctuation throughout)\n",
    "\n",
    "Notes: \n",
    "* Sigmoids are not used often because it has a small maximum derivative value and thus propagates only a small amount of error each time, leading to slow \"learning\" \n",
    "* This small-derivative-slow-learning issue is known as a **vanishing gradient** problem\n",
    "* Tanh is mathematically quite similar to a sigmoid function, thus also has the vanishing gradient issue, but not as bad\n",
    "* ReLu generally works well because its gradient is always 1, as long as the input is positive (no vanishing gradients), and negative inputs going to 0 can make your network lighter (no weights/biases are being updated)\n",
    "\n",
    "Here are resources on both the [vanishing gradient](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/m) and [exploding gradient](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Loss functions\n",
    "\n",
    "Loss functions are akin to cost functions we were trying to minimize in gradient descent (i.e. RMSE for linear regression, Gini/entropy for trees)\n",
    "\n",
    "1. For regression problems, keras has **mean_squared_error** or **mean_absolute_error** as a loss function, or **mean_squared_logarithmic_error** if your target has potential outliers\n",
    "2. For binary classification: **binary_crossentropy** \n",
    "3. For multiclass problems: **categorical_crossentropy**\n",
    "\n",
    "[This article summarizes the above, and more.](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Optimizers\n",
    "\n",
    "Overview Resource: \n",
    "- [A (Quick) Guide to Neural Network Optimizers with Applications in Keras](https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4) blog post - an overview of the options\n",
    "\n",
    "Summary:\n",
    "* Different optimizers are just different methods/paths that your neural network can take to find optimal values\n",
    "* Experimentally, Adam (derived from *adaptive moment estimation*) is a good one to use - [here's more about Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "#### Quick optimizer summary\n",
    "\n",
    "* **RMSProp**: maintains per-parameter learning rates adapted based on the average of recent weight updates (e.g. how quickly it is changing). This does well on non-stationary problems (e.g. noisy data)\n",
    "\n",
    "* **Adagrad**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems)\n",
    "\n",
    "* **Adam**: realizes the benefits of both AdaGrad and RMSProp. Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Learning rate\n",
    "\n",
    "* The learning rate is something you can define when you compile your model with the optimizer\n",
    "* Optimizers usually change up the learning rates, so this is just the *initial* learning rate\n",
    "* If you set it too low, training will eventually converge, but it will do so slowly\n",
    "* If you set it too high, it might actually diverge\n",
    "* If you set it slightly too high, it will converge at first but miss the local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Regularization\n",
    "\n",
    "* As a neural network learns, neuron weights settle into their context within the network\n",
    "* Weights of neurons are tuned for specific features providing some specialization\n",
    "* Neighboring neurons become too reliant on this specialization, which if taken too far can result in a fragile model too specialized to the training data\n",
    "* This reliance on context for a neuron during training is referred to as *complex co-adaptations*\n",
    "\n",
    "#### Methods\n",
    "1. You can add L1 or L2 regularization within each hidden layer\n",
    "2. You can also add a **dropout layer** \n",
    "3. Not technically *regularization*, but you can introduce **early stopping** so your model doesn't overtrain\n",
    "\n",
    "#### Dropout\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. You can add **dropout layers** in your neural network.\n",
    "\n",
    "<img src='images/thanos.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Text Classification\n",
    "\n",
    "## Pre-Trained Networks and Word Embeddings and LSTMs, oh my!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More specific imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/IMDB_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target value\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Split Preprocessing\n",
    "\n",
    "Doing some initial preprocessing that can be done before the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check out an example review...\n",
    "index_num = 15903 # Defining the index number of the review to explore\n",
    "\n",
    "df['review'].iloc[index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some HTML tags inside these texts... will want to remove them. But how?\n",
    "\n",
    "Enter: Regular Expressions (regex).\n",
    "\n",
    "Testing: https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the pattern to remove html tags\n",
    "import re\n",
    "\n",
    "html_tag_pattern = re.compile(r'<[^>]*>')\n",
    "\n",
    "test = html_tag_pattern.sub('', df['review'].iloc[index_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our pattern to the dataset\n",
    "df['review'] = df['review'].map(lambda x: re.sub(r'<[^>]*>', '', x))\n",
    "\n",
    "# Same as\n",
    "# df['review'] = df['review'].map(lambda x: html_tag_pattern.sub('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "df['review'].iloc[index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neat bit of code!\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word.lower() not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also pre-process our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target map\n",
    "target_map = {'positive': 1,\n",
    "              'negative': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map it\n",
    "df['sentiment'] = df['sentiment'].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split, and then Post-Split Processing\n",
    "Now let's perform a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find that same review now that the index is shuffled\n",
    "train_index_num = X_train.index.get_loc(15903)\n",
    "X_train.iloc[train_index_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Text Classification... What Would We Do?\n",
    "\n",
    "Aka what would this look like without a NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a TF-IDF vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What parameters should we set? What steps have we already done, what do we still need to do?\n",
    "# Already removed stopwords!\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.95,  # removes words that appear in more than 95% of docs\n",
    "    min_df=2 # removes words that appear 2 or fewer times\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Our Vectorized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's look at that second example again\n",
    "X_train.iloc[train_index_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[X_train.str.contains('CHILDREN\\'S MOVIE!!!')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df of tf-idf values, where each column is a word in the vocabulary\n",
    "tfidf_train_df = pd.DataFrame(X_train_vec.toarray(), \n",
    "                              columns=vectorizer.get_feature_names(), \n",
    "                              index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing that row once it's been vectorized\n",
    "test_doc = tfidf_train_df.iloc[train_index_num]\n",
    "\n",
    "test_doc[test_doc > 0].sort_values(ascending=False).head(15) # Showing values > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell you about the word \"censure\" in the this document?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "classifier.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to NN-Based Text Classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Pre-Processing Steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through these steps first, then discuss why we didn't just use vectorized text.\n",
    "\n",
    "Going to use keras's tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find our longest review - will need for padding later\n",
    "max_length = max([len(s.split()) for s in X_train])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to tokenize\n",
    "# Recommend checking out their default values - they're removing punctuation for us!\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this doing? \n",
    "#Let's look at the first 10 key-value pairs in the word_index dict\n",
    "\n",
    "list(tokenizer.word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same example, after processing\n",
    "print(X_train_token[train_index_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the corpus size\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's pad so each review is the same length as our longest review\n",
    "# Basically, adding zeros at the end\n",
    "\n",
    "X_train_processed = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_token, maxlen=max_length, padding='post')\n",
    "X_test_processed = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_token, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train_processed[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Couldn't We Just Use TF-IDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the vectorized text\n",
    "tfidf_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the preprocessed text from keras\n",
    "X_train_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference? Specifically, what are the columns representing in each of these? What are the numbers?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now: Using Pre-Trained Embeddings and NNs for NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, following this example: https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n",
    "\n",
    "Also relevant: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Word Embeddings: GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "The link to download the GloVe files: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "> **You will need to download the GloVe embeddings directly, since these files are all too big for github!**\n",
    "\n",
    "The below function and code comes from: https://realpython.com/python-keras-text-classification/#using-pretrained-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(glove_filepath, word_index, embedding_dim):\n",
    "    '''\n",
    "    Grabs the embeddings just for the words in our vocabulary\n",
    "    \n",
    "    Inputs:\n",
    "    glove_filepath - string, location of the glove text file to use\n",
    "    word_index - word_index attribute from the keras tokenizer\n",
    "    embedding_dim - int, number of dimensions to embed, a hyperparameter\n",
    "    \n",
    "    Output:\n",
    "    embedding_matrix - numpy array of embeddings\n",
    "    '''\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(glove_filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt',\n",
    "                                           tokenizer.word_index, \n",
    "                                           embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from previous preprocessing steps?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time to model!\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=max_length, \n",
    "                           trainable=False)) # Note - not retraining the embedding layer\n",
    "model.add(layers.Flatten()) # flattening these layers down before connecting to dense layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_processed, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Embeddings as Starting Weights, but Allow Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to model!\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=max_length, \n",
    "                           trainable=True)) # Now it can retrain the embedding layer\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test))\n",
    "# Takes about... 3 minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_processed, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Patience: how many epochs that model can keep running without improvement before the training is stopped\n",
    "\n",
    "Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with a model saving feature, so it saves as it improves\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model as just before this\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just adding more epochs\n",
    "history = model.fit(X_train_processed, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test_processed, y_test),\n",
    "                    callbacks=[es, mc])\n",
    "\n",
    "# This takes... a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Note: there might still be a bug in tensorflow related to the newest numpy version, if you have numpy version 1.20+ this might not work.\n",
    "\n",
    "https://github.com/tensorflow/models/issues/9706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=max_length,\n",
    "                           trainable=True))\n",
    "# Replacing our Flattening layer with an LSTM\n",
    "# Adding some dropout to prevent overfitting - note the two ways to do so\n",
    "model.add(layers.LSTM(embedding_dim, \n",
    "                      dropout=0.2,\n",
    "                      return_sequences=True))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do this all here... or we could move over to Kaggle and run this with GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('model.h5')\n",
    "my_model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.evaluate(X_test_processed.values, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Sklearn's [Working with Text Data Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "What else can we do with natural language data beyond text classification? \n",
    "\n",
    "- [This blog post](https://blog.aureusanalytics.com/blog/5-natural-language-processing-techniques-for-extracting-information) by Aureus Analytics provides an overview of other machine learning techniques used to extract meaning from text: Named Entity Recognition, Sentiment Analysis, Text Summarization, Aspect Mining and Topic Modeling\n",
    "\n",
    "### Neural Network Vectorizer Resources:\n",
    "\n",
    "Want another way to embed words for machine learning? Check out Word2Vec - a way of vectorizing text that tries to capture the relationships between words. See the image below, from [this paper](https://arxiv.org/pdf/1310.4546.pdf) from Google developers, that introduced a Skip-gram neural network model that's been utilized by Word2Vec (which is a tool you can use to implement this model). You'll note that the distance between each country and it's capital city is about the same - that distance actually has meaning, and thus you can imagine that the difference between `cat` and `kitten` would be the same as the difference between `dog` and `puppy`. Et cetera!\n",
    "\n",
    "![screenshot from a paper on the Skip-gram model from devleopers at Google, https://arxiv.org/pdf/1310.4546.pdf](images/Fig2-DsitributedRepresentationsOfWordsAndPhrasesAndTheirCompositionality.png)\n",
    "\n",
    "- [Pathmind's A.I. Wiki - A Beginner's Guide to Word2Vec](https://wiki.pathmind.com/word2vec)\n",
    "- [Chris McCormick's Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [What is the difference between Word2Vec and GloVe?](https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 2: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New dataset!\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "plt.show()\n",
    "# If you get a \"downloading data\" thing, worry not, shouldn't take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the class balance of our data\n",
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to prep our data\n",
    "# Scale images to the [0, 1] range - max is 255\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (32, 32, 3)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So - What is our Input Shape? What is our Output Shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3) # Size of each image - 32x32 for 3 layers\n",
    "output_shape = 10 # Number of classes, aka number of possible targets (multi class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to prep our outputs\n",
    "# Different from binary classification!\n",
    "y_train = keras.utils.to_categorical(y_train, output_shape)\n",
    "y_test = keras.utils.to_categorical(y_test, output_shape)\n",
    "\n",
    "print(\"y_train shape:\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First - Simple Multi-Layer Perceptron!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building in a single list\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape), # Don't always need this input separately\n",
    "        layers.Flatten(), # need to flatten our images to be one long array\n",
    "        layers.Dense(64, activation=\"tanh\"),\n",
    "        layers.Dense(output_shape, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(X_train.shape[0]/20)\n",
    "epochs = 15\n",
    "\n",
    "# Compiling our model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model (save output to a history variable)\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN - AKA Incorporating Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation.\n",
    "\n",
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "2. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "After a convolutional layer, the feature maps are fed into a max pool layer. Like convolutions, this method is applied one patch at a time (usually 2x2). Max pooling simply takes the largest value from one patch of an image, places it in a new matrix next to the max values from other patches, and discards the rest of the information contained in the activation maps. Other methods exist such as average pooling (taking an average of the patch).\n",
    "\n",
    "<img src='images/maxpool.png'/>\n",
    "\n",
    "This process results in a new feature map with reduced dimensionality that is then passed into another convolution layer to continue the pattern finding process. These steps are repeated until they are passed to a fully connected layer that proceeds to classify the image using the identified patterns.\n",
    "\n",
    "### Flattening\n",
    "\n",
    "Once the neural network has collected a series of patterns that an image contains, it is ready to make a guess as to what the image is. In order to do so, it starts by **flattening** the 2D matrix into a 1D vector, so it can be passed into a normal densely connected layer for classification. Then using this vector, one or many densely connected layers will make a prediction as to what the image is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for this dataset/NN architecture: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building using \"add\"\n",
    "\n",
    "cnn = keras.Sequential()\n",
    "# We defined a variable input_shape earlier, can use that here\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# now, to get the proper output\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu'))\n",
    "cnn.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=2, # small since this is just a demo\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_test, y_test))\n",
    "# note - even with just 2 epochs this'll take at least a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A worked-through example on this dataset: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretrained network (also known as a convolutional base for CNNs) consists of layers that have already been trained on typically general data. For images, these layers have already learned general patterns, textures, colors, etc. such that when you feed in your training data, certain features can immediately be detected. This part is **feature extraction**.\n",
    "\n",
    "You typically add your own final layers to train the network to classify/regress based on your problem. This component is **fine tuning**\n",
    "\n",
    "Here are the pretrained image classification models that exist within Keras: https://keras.io/api/applications/\n",
    "\n",
    "VGG19: https://keras.io/api/applications/vgg/#vgg19-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = VGG19(weights='imagenet',\n",
    "                   include_top=False, # Allows us to set input shape\n",
    "                   input_shape=input_shape) \n",
    "# May download data at this step, shouldn't take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential()\n",
    "cnn.add(pretrained)\n",
    "\n",
    "# freezing layers so they don't get re-trained with your new data\n",
    "for layer in cnn.layers:\n",
    "    layer.trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our own dense layers\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu'))\n",
    "cnn.add(layers.Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to verify that the weights are \"frozen\" \n",
    "for layer in cnn.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this you can now compile and fit your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train_resized,\n",
    "                  y_train,\n",
    "                  epochs=5, \n",
    "                  batch_size=64,\n",
    "                  validation_data=(X_test_resized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
